{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eee4da8-a2be-435e-8756-52f8b0428612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import pandas as pd\n",
    "from hazm import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94e4db97-6840-42a1-bd6e-f25dcf0743c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_elements_by_attribute(root, attribute_name, attribute_value):\n",
    "    return root.findall(\".//*[@{}='{}']\".format(attribute_name, attribute_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0515b646-12d8-4465-8064-b0d6c887cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_folders(directory):\n",
    "    folders = []\n",
    "    for item in os.listdir(directory):\n",
    "        if os.path.isdir(os.path.join(directory, item)):\n",
    "            folders.append(item)\n",
    "    return folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3377b00-2c64-47c1-a626-645b6017a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrong_correct_sentence(root,begin,end,correct_word):\n",
    "    wrong_sentence , correct_sentence=\"\",\"\"\n",
    "    for element in root.findall(\"{http:///de/tudarmstadt/ukp/dkpro/core/api/segmentation/type.ecore}Sentence\"):\n",
    "        if int(element.get(\"begin\"))<= begin and int(element.get(\"end\"))>=end:\n",
    "            wrong_sentence=root.findall(\"{http:///uima/cas.ecore}Sofa\")[0].get(\"sofaString\")[int(element.get(\"begin\")):int(element.get(\"end\"))] \n",
    "            s_begin=int(element.get(\"begin\"))\n",
    "            s_end=int(element.get(\"end\"))\n",
    "            if correct_word==None:\n",
    "                correct_word=\" \"\n",
    "            correct_sentence=root.findall(\"{http:///uima/cas.ecore}Sofa\")[0].get(\"sofaString\")[s_begin:begin]+correct_word+root.findall(\"{http:///uima/cas.ecore}Sofa\")[0].get(\"sofaString\")[end:s_end] \n",
    "\n",
    "    return wrong_sentence, correct_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca31bd2e-02be-4eef-b156-19e97d109e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\n",
    "    'file_name':[],\n",
    "    'USER_NUMBER':[],\n",
    "    'begin':[],\n",
    "    'end':[],\n",
    "    'wrong_word':[],\n",
    "    'correct_word':[],\n",
    "    'wrong_sentence':[],\n",
    "    'correct_sentence':[],\n",
    "    'process':[],\n",
    "    'main_category':[],\n",
    "    'sub_category_1':[],\n",
    "    'sub_category_2':[]\n",
    "\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43c83cc4-a6f7-41d1-a249-43512c12c682",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors={\n",
    "    \"Main signs\":1,\n",
    "    \"Diacritic signs\":2,\n",
    "    \"Form\":3,\n",
    "    \"Punctuation\":4,\n",
    "    None:5\n",
    "}\n",
    "sub_category_1={\n",
    "    None:0,\n",
    "    \"None\":0,\n",
    "    \"Boundary\":1,\n",
    "    \"Consonants\":2,\n",
    "    \"Vowels\":3,\n",
    "    \"Tashdid\":4,\n",
    "    \"Madd\":5,\n",
    "    \"Tanwin\":6,\n",
    "    \"Alif Lam\":7,\n",
    "    \"Place\":8,\n",
    "    \"Dot\":9\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c2e206e-34e0-44c4-9f1c-088d48e3cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_XMI(path,USER_NUMBER):\n",
    "    data_temp={\n",
    "    'file_name':[],\n",
    "    'USER_NUMBER':[],\n",
    "    'begin':[],\n",
    "    'end':[],\n",
    "    'wrong_word':[],\n",
    "    'correct_word':[],\n",
    "    'wrong_sentence':[],\n",
    "    'correct_sentence':[],\n",
    "    'process':[],\n",
    "    'main_category':[],\n",
    "    'sub_category_1':[],\n",
    "    'sub_category_2':[]\n",
    "}\n",
    "    tree = ET.parse(path)\n",
    "    root=tree.getroot()\n",
    "    if len(root.findall('{http:///webanno/custom.ecore}Orthography'))!=0 :\n",
    "        # print(root.findall(\"{http:///uima/cas.ecore}Sofa\")[0].get(\"sofaString\"))\n",
    "        for element in root.findall('{http:///webanno/custom.ecore}Orthography'):\n",
    "            data_temp['USER_NUMBER'].append(USER_NUMBER)\n",
    "            begin=element.get(\"begin\")\n",
    "            end=element.get(\"end\")\n",
    "            data_temp['file_name'].append(root.findall(\"{http:///de/tudarmstadt/ukp/dkpro/core/api/metadata/type.ecore}DocumentMetaData\")[0].get(\"documentTitle\"))\n",
    "            data_temp['begin'].append(begin)\n",
    "            data_temp['end'].append(end)\n",
    "            data_temp['correct_word'].append(element.get(\"correction\"))\n",
    "            data_temp['main_category'].append(element.get(\"mainCategory\"))\n",
    "            data_temp['sub_category_1'].append(element.get(\"subCategory_1\"))\n",
    "            data_temp['wrong_word'].append(root.findall(\"{http:///uima/cas.ecore}Sofa\")[0].get(\"sofaString\")[int(begin):int(end)])\n",
    "            data_temp['process'].append(element.get(\"process\"))\n",
    "            if \"subCategory_2\" in element.attrib:\n",
    "                data_temp[\"sub_category_2\"].append(element.get(\"subCategory_2\"))\n",
    "            else:\n",
    "                data_temp[\"sub_category_2\"].append(\"None\")\n",
    "            wrong_sentence,correct_sentence=wrong_correct_sentence(root,int(begin),int(end),element.get(\"correction\"))\n",
    "            data_temp[\"wrong_sentence\"].append(wrong_sentence)\n",
    "            data_temp[\"correct_sentence\"].append(correct_sentence)\n",
    "\n",
    "    df2=pd.DataFrame(data_temp)\n",
    "    return df2\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cee451a-b57c-4289-9977-32c6204a89ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folders=list_all_folders(\"./all_data\")\n",
    "for folder in annotation_folders:\n",
    "    all_items = os.listdir(\"./all_data/\"+folder)\n",
    "    xmi_files = [f for f in all_items if f.endswith('.xmi') and os.path.isfile(os.path.join(\"./all_data\",folder, f))]\n",
    "    for xmi_file in xmi_files:\n",
    "        xmi_path=os.path.join(\"./all_data\",folder,xmi_file)\n",
    "    output= extract_data_from_XMI(xmi_path,xmi_file[:-4])\n",
    "    if len(output)!=0:\n",
    "        df=pd.concat([df,output],axis=0,ignore_index=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a7f26f1-1a7f-41e7-a4f5-e0403cffa909",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens_wrong=[]\n",
    "word_tokens_correct=[]\n",
    "error_index=[]\n",
    "for index , row in df.iterrows():\n",
    "    row['wrong_sentence']=' '.join(row['wrong_sentence'].split())\n",
    "    row['correct_sentence']=' '.join(row['correct_sentence'].split())\n",
    "    wrong_tokens=word_tokenize(row['wrong_sentence'])\n",
    "    zeros=[0]*len(wrong_tokens)\n",
    "    word_tokens_correct.append(word_tokenize(row['correct_sentence']))\n",
    "    word_tokens_wrong.append(wrong_tokens)\n",
    "    for i in range(0,len(wrong_tokens)):\n",
    "        if row['wrong_word']==wrong_tokens[i]:\n",
    "            zeros[i]=errors[row['main_category']]*10+sub_category_1[row['sub_category_1']]\n",
    "    error_index.append(zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed7945-ac54-49be-81b5-9d97fb3eda67",
   "metadata": {},
   "source": [
    "word_tokens_correct=[]\n",
    "error_vector=[]\n",
    "for index, row in df.iterrows():\n",
    "    row['wrong_sentence']=' '.join(row['wrong_sentence'].split())\n",
    "    tokens=word_tokenize(row['wrong_sentence'])\n",
    "    zeros=[0]*len(tokens)\n",
    "    word_tokens_wrong.append(tokens)\n",
    "    for i in range(0,len(tokens)):\n",
    "        if row['wrong_word']==tokens[i]:\n",
    "            zeros[i]=errors[row['main_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d5fffcc-0a16-4a6c-bd22-c9eb2b4658c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_tokens_wrong']=word_tokens_wrong\n",
    "df['word_tokens_correct']=word_tokens_correct\n",
    "df['error_index']=error_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45bec816-27c5-446d-bfe4-84bcd4056255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>USER_NUMBER</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>wrong_word</th>\n",
       "      <th>correct_word</th>\n",
       "      <th>wrong_sentence</th>\n",
       "      <th>correct_sentence</th>\n",
       "      <th>process</th>\n",
       "      <th>main_category</th>\n",
       "      <th>sub_category_1</th>\n",
       "      <th>sub_category_2</th>\n",
       "      <th>word_tokens_wrong</th>\n",
       "      <th>word_tokens_correct</th>\n",
       "      <th>error_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n_dialog_11_3.txt</td>\n",
       "      <td>4011744020</td>\n",
       "      <td>140</td>\n",
       "      <td>145</td>\n",
       "      <td>داداش</td>\n",
       "      <td>داداش؟</td>\n",
       "      <td>سلام، چطوری داداش الحمدلله خوبم انشاء الله</td>\n",
       "      <td>سلام، چطوری داداش؟ الحمدلله خوبم انشاء الله</td>\n",
       "      <td>Omission</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[سلام, ،, چطوری, داداش, الحمدلله, خوبم, انشاء,...</td>\n",
       "      <td>[سلام, ،, چطوری, داداش, ؟, الحمدلله, خوبم, انش...</td>\n",
       "      <td>[0, 0, 0, 40, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n_dialog_11_3.txt</td>\n",
       "      <td>4011744020</td>\n",
       "      <td>181</td>\n",
       "      <td>185</td>\n",
       "      <td>خوبم</td>\n",
       "      <td>خوبم.</td>\n",
       "      <td>سلام، چطوری داداش الحمدلله خوبم انشاء الله</td>\n",
       "      <td>سلام، چطوری داداش الحمدلله خوبم. انشاء الله</td>\n",
       "      <td>Replacement</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[سلام, ،, چطوری, داداش, الحمدلله, خوبم, انشاء,...</td>\n",
       "      <td>[سلام, ،, چطوری, داداش, الحمدلله, خوبم, ., انش...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 40, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n_dialog_11_3.txt</td>\n",
       "      <td>4011744020</td>\n",
       "      <td>186</td>\n",
       "      <td>191</td>\n",
       "      <td>انشاء</td>\n",
       "      <td>ان شاء</td>\n",
       "      <td>سلام، چطوری داداش الحمدلله خوبم انشاء الله</td>\n",
       "      <td>سلام، چطوری داداش الحمدلله خوبم ان شاء الله</td>\n",
       "      <td>Replacement</td>\n",
       "      <td>Form</td>\n",
       "      <td>Boundary</td>\n",
       "      <td>Merging</td>\n",
       "      <td>[سلام, ،, چطوری, داداش, الحمدلله, خوبم, انشاء,...</td>\n",
       "      <td>[سلام, ،, چطوری, داداش, الحمدلله, خوبم, ان, شا...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 31, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n_dialog_11_3.txt</td>\n",
       "      <td>4011744020</td>\n",
       "      <td>198</td>\n",
       "      <td>202</td>\n",
       "      <td>خوبی</td>\n",
       "      <td>خوبی؟</td>\n",
       "      <td>خوبی سلامتی انشاءالله شما هم خوب هستید</td>\n",
       "      <td>خوبی؟ سلامتی انشاءالله شما هم خوب هستید</td>\n",
       "      <td>Omission</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[خوبی, سلامتی, انشاءالله, شما, هم, خوب, هستید]</td>\n",
       "      <td>[خوبی, ؟, سلامتی, انشاءالله, شما, هم, خوب, هستید]</td>\n",
       "      <td>[40, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n_dialog_11_3.txt</td>\n",
       "      <td>4011744020</td>\n",
       "      <td>203</td>\n",
       "      <td>209</td>\n",
       "      <td>سلامتی</td>\n",
       "      <td>سلامتی؟</td>\n",
       "      <td>خوبی سلامتی انشاءالله شما هم خوب هستید</td>\n",
       "      <td>خوبی سلامتی؟ انشاءالله شما هم خوب هستید</td>\n",
       "      <td>Omission</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[خوبی, سلامتی, انشاءالله, شما, هم, خوب, هستید]</td>\n",
       "      <td>[خوبی, سلامتی, ؟, انشاءالله, شما, هم, خوب, هستید]</td>\n",
       "      <td>[0, 40, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           file_name USER_NUMBER begin  end wrong_word correct_word  \\\n",
       "0  n_dialog_11_3.txt  4011744020   140  145      داداش       داداش؟   \n",
       "1  n_dialog_11_3.txt  4011744020   181  185       خوبم        خوبم.   \n",
       "2  n_dialog_11_3.txt  4011744020   186  191      انشاء       ان شاء   \n",
       "3  n_dialog_11_3.txt  4011744020   198  202       خوبی        خوبی؟   \n",
       "4  n_dialog_11_3.txt  4011744020   203  209     سلامتی      سلامتی؟   \n",
       "\n",
       "                               wrong_sentence  \\\n",
       "0  سلام، چطوری داداش الحمدلله خوبم انشاء الله   \n",
       "1  سلام، چطوری داداش الحمدلله خوبم انشاء الله   \n",
       "2  سلام، چطوری داداش الحمدلله خوبم انشاء الله   \n",
       "3      خوبی سلامتی انشاءالله شما هم خوب هستید   \n",
       "4      خوبی سلامتی انشاءالله شما هم خوب هستید   \n",
       "\n",
       "                              correct_sentence      process main_category  \\\n",
       "0  سلام، چطوری داداش؟ الحمدلله خوبم انشاء الله     Omission   Punctuation   \n",
       "1  سلام، چطوری داداش الحمدلله خوبم. انشاء الله  Replacement   Punctuation   \n",
       "2  سلام، چطوری داداش الحمدلله خوبم ان شاء الله  Replacement          Form   \n",
       "3      خوبی؟ سلامتی انشاءالله شما هم خوب هستید     Omission   Punctuation   \n",
       "4      خوبی سلامتی؟ انشاءالله شما هم خوب هستید     Omission   Punctuation   \n",
       "\n",
       "  sub_category_1 sub_category_2  \\\n",
       "0           None           None   \n",
       "1           None           None   \n",
       "2       Boundary        Merging   \n",
       "3           None           None   \n",
       "4           None           None   \n",
       "\n",
       "                                   word_tokens_wrong  \\\n",
       "0  [سلام, ،, چطوری, داداش, الحمدلله, خوبم, انشاء,...   \n",
       "1  [سلام, ،, چطوری, داداش, الحمدلله, خوبم, انشاء,...   \n",
       "2  [سلام, ،, چطوری, داداش, الحمدلله, خوبم, انشاء,...   \n",
       "3     [خوبی, سلامتی, انشاءالله, شما, هم, خوب, هستید]   \n",
       "4     [خوبی, سلامتی, انشاءالله, شما, هم, خوب, هستید]   \n",
       "\n",
       "                                 word_tokens_correct  \\\n",
       "0  [سلام, ،, چطوری, داداش, ؟, الحمدلله, خوبم, انش...   \n",
       "1  [سلام, ،, چطوری, داداش, الحمدلله, خوبم, ., انش...   \n",
       "2  [سلام, ،, چطوری, داداش, الحمدلله, خوبم, ان, شا...   \n",
       "3  [خوبی, ؟, سلامتی, انشاءالله, شما, هم, خوب, هستید]   \n",
       "4  [خوبی, سلامتی, ؟, انشاءالله, شما, هم, خوب, هستید]   \n",
       "\n",
       "                 error_index  \n",
       "0  [0, 0, 0, 40, 0, 0, 0, 0]  \n",
       "1  [0, 0, 0, 0, 0, 40, 0, 0]  \n",
       "2  [0, 0, 0, 0, 0, 0, 31, 0]  \n",
       "3     [40, 0, 0, 0, 0, 0, 0]  \n",
       "4     [0, 40, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b67da00e-fa15-4a32-8529-abe4d173bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data_.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e779b282-09c9-478d-a7b4-f2d9648f4d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('data_.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4cfe19-79f0-4d06-9f75-7ec25300e3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
