{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eee4da8-a2be-435e-8756-52f8b0428612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import pandas as pd\n",
    "from hazm import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e4db97-6840-42a1-bd6e-f25dcf0743c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_elements_by_attribute(root, attribute_name, attribute_value):\n",
    "    return root.findall(\".//*[@{}='{}']\".format(attribute_name, attribute_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0515b646-12d8-4465-8064-b0d6c887cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_folders(directory):\n",
    "    folders = []\n",
    "    for item in os.listdir(directory):\n",
    "        if os.path.isdir(os.path.join(directory, item)):\n",
    "            folders.append(item)\n",
    "    return folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3377b00-2c64-47c1-a626-645b6017a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrong_correct_sentence(root,begin,end,correct_word):\n",
    "    wrong_sentence , correct_sentence=\"\",\"\"\n",
    "    for element in root.findall(\"{http:///de/tudarmstadt/ukp/dkpro/core/api/segmentation/type.ecore}Sentence\"):\n",
    "        if int(element.get(\"begin\"))<= begin and int(element.get(\"end\"))>=end:\n",
    "            wrong_sentence=root.findall(\"{http:///uima/cas.ecore}Sofa\")[0].get(\"sofaString\")[int(element.get(\"begin\")):int(element.get(\"end\"))] \n",
    "            s_begin=int(element.get(\"begin\"))\n",
    "            s_end=int(element.get(\"end\"))\n",
    "            if correct_word==None:\n",
    "                correct_word=\" \"\n",
    "            correct_sentence=root.findall(\"{http:///uima/cas.ecore}Sofa\")[0].get(\"sofaString\")[s_begin:begin]+correct_word+root.findall(\"{http:///uima/cas.ecore}Sofa\")[0].get(\"sofaString\")[end:s_end] \n",
    "\n",
    "    return wrong_sentence, correct_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca31bd2e-02be-4eef-b156-19e97d109e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\n",
    "    'file_name':[],\n",
    "    'USER_NUMBER':[],\n",
    "    'begin':[],\n",
    "    'end':[],\n",
    "    'wrong_word':[],\n",
    "    'correct_word':[],\n",
    "    'wrong_sentence':[],\n",
    "    'correct_sentence':[],\n",
    "    'process':[],\n",
    "    'main_category':[],\n",
    "    'sub_category_1':[],\n",
    "    'sub_category_2':[]\n",
    "\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43c83cc4-a6f7-41d1-a249-43512c12c682",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors={\n",
    "    \"Main signs\":1,\n",
    "    \"Diacritic signs\":2,\n",
    "    \"Form\":3,\n",
    "    \"Punctuation\":4,\n",
    "    None:5\n",
    "}\n",
    "sub_category_1={\n",
    "    None:0,\n",
    "    \"None\":0,\n",
    "    \"Boundary\":1,\n",
    "    \"Consonants\":2,\n",
    "    \"Vowels\":3,\n",
    "    \"Tashdid\":4,\n",
    "    \"Madd\":5,\n",
    "    \"Tanwin\":6,\n",
    "    \"Alif Lam\":7,\n",
    "    \"Place\":8,\n",
    "    \"Dot\":9\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c2e206e-34e0-44c4-9f1c-088d48e3cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_XMI(path,USER_NUMBER):\n",
    "    data_temp={\n",
    "    'file_name':[],\n",
    "    'USER_NUMBER':[],\n",
    "    'begin':[],\n",
    "    'end':[],\n",
    "    'wrong_word':[],\n",
    "    'correct_word':[],\n",
    "    'wrong_sentence':[],\n",
    "    'correct_sentence':[],\n",
    "    'process':[],\n",
    "    'main_category':[],\n",
    "    'sub_category_1':[],\n",
    "    'sub_category_2':[]\n",
    "}\n",
    "    tree = ET.parse(path)\n",
    "    root=tree.getroot()\n",
    "    if len(root.findall('{http:///webanno/custom.ecore}Orthography'))!=0 :\n",
    "        # print(root.findall(\"{http:///uima/cas.ecore}Sofa\")[0].get(\"sofaString\"))\n",
    "        for element in root.findall('{http:///webanno/custom.ecore}Orthography'):\n",
    "            data_temp['USER_NUMBER'].append(USER_NUMBER)\n",
    "            begin=element.get(\"begin\")\n",
    "            end=element.get(\"end\")\n",
    "            data_temp['file_name'].append(root.findall(\"{http:///de/tudarmstadt/ukp/dkpro/core/api/metadata/type.ecore}DocumentMetaData\")[0].get(\"documentTitle\"))\n",
    "            data_temp['begin'].append(begin)\n",
    "            data_temp['end'].append(end)\n",
    "            data_temp['correct_word'].append(element.get(\"correction\"))\n",
    "            data_temp['main_category'].append(element.get(\"mainCategory\"))\n",
    "            data_temp['sub_category_1'].append(element.get(\"subCategory_1\"))\n",
    "            data_temp['wrong_word'].append(root.findall(\"{http:///uima/cas.ecore}Sofa\")[0].get(\"sofaString\")[int(begin):int(end)])\n",
    "            data_temp['process'].append(element.get(\"process\"))\n",
    "            if \"subCategory_2\" in element.attrib:\n",
    "                data_temp[\"sub_category_2\"].append(element.get(\"subCategory_2\"))\n",
    "            else:\n",
    "                data_temp[\"sub_category_2\"].append(\"None\")\n",
    "            wrong_sentence,correct_sentence=wrong_correct_sentence(root,int(begin),int(end),element.get(\"correction\"))\n",
    "            data_temp[\"wrong_sentence\"].append(wrong_sentence)\n",
    "            data_temp[\"correct_sentence\"].append(correct_sentence)\n",
    "\n",
    "    df2=pd.DataFrame(data_temp)\n",
    "    return df2\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cee451a-b57c-4289-9977-32c6204a89ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folders=list_all_folders(\"./all_data\")\n",
    "for folder in annotation_folders:\n",
    "    all_items = os.listdir(\"./all_data/\"+folder)\n",
    "    xmi_files = [f for f in all_items if f.endswith('.xmi') and os.path.isfile(os.path.join(\"./all_data\",folder, f))]\n",
    "    for xmi_file in xmi_files:\n",
    "        xmi_path=os.path.join(\"./all_data\",folder,xmi_file)\n",
    "    output= extract_data_from_XMI(xmi_path,xmi_file[:-4])\n",
    "    if len(output)!=0:\n",
    "        df=pd.concat([df,output],axis=0,ignore_index=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a7f26f1-1a7f-41e7-a4f5-e0403cffa909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بعد از ضهربرای تفریخ و کمک به پدربزرگم به باغ رفتم .\n",
      "file_name                                       n_dictation_11_25.txt\n",
      "USER_NUMBER                                                4011714026\n",
      "begin                                                             279\n",
      "end                                                               289\n",
      "wrong_word                                                 بعد از ضهر\n",
      "correct_word                                                 بعدازظهر\n",
      "wrong_sentence      بعد از ضهربرای تفریخ و کمک به پدربزرگم به باغ ...\n",
      "correct_sentence    بعدازظهربرای تفریخ و کمک به پدربزرگم به باغ رف...\n",
      "process                                                   Replacement\n",
      "main_category                                                    Form\n",
      "sub_category_1                                               Boundary\n",
      "sub_category_2                                                Merging\n",
      "Name: 1494, dtype: object\n",
      "\n",
      "این مشکل مخصوصا در کشورهای غربی ، مانند آمریکا و بعضی از کش . رهای اورپا دیده می شود .\n",
      "file_name                                            SFLC_LC_1807.txt\n",
      "USER_NUMBER                                                4001744016\n",
      "begin                                                              94\n",
      "end                                                                98\n",
      "wrong_word                                                     کش . ر\n",
      "correct_word                                                     کشور\n",
      "wrong_sentence      این مشکل مخصوصا در کشورهای غربی ، مانند آمریکا...\n",
      "correct_sentence    این مشکل مخصوصا در کشورهای غربی ، مانند آمریکا...\n",
      "process                                                   Replacement\n",
      "main_category                                              Main signs\n",
      "sub_category_1                                             Consonants\n",
      "sub_category_2                                                      و\n",
      "Name: 5440, dtype: object\n",
      "\n",
      "\n",
      "file_name                                 z_essay_18_0.txt\n",
      "USER_NUMBER                                      981714019\n",
      "begin                                                  228\n",
      "end                                                    264\n",
      "wrong_word          مامان من جا می روم . با شما عالی است .\n",
      "correct_word         مامان من هرجا می روم با شما عالی است.\n",
      "wrong_sentence                                            \n",
      "correct_sentence                                          \n",
      "process                                                ADD\n",
      "main_category                                  Punctuation\n",
      "sub_category_1                                        None\n",
      "sub_category_2                                        None\n",
      "Name: 9293, dtype: object\n",
      "\n",
      "\n",
      "file_name                                           z_essay_18_27.txt\n",
      "USER_NUMBER                                                 981714019\n",
      "begin                                                             523\n",
      "end                                                               613\n",
      "wrong_word          ( السلات ، الزکات ، الحاج ، الخرمس ، الجحاد ، ...\n",
      "correct_word        «الصّلات، الزّکات، الحجّ، الخمس، الجهاد، الأمر...\n",
      "wrong_sentence                                                       \n",
      "correct_sentence                                                     \n",
      "process                                                   Replacement\n",
      "main_category                                             Punctuation\n",
      "sub_category_1                                                   None\n",
      "sub_category_2                                                   None\n",
      "Name: 9643, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_tokens_wrong=[]\n",
    "word_tokens_correct=[]\n",
    "error_index=[]\n",
    "for index , row in df.iterrows():\n",
    "    row['wrong_sentence']=' '.join(word_tokenize(row['wrong_sentence']))\n",
    "    row['correct_sentence']=' '.join(word_tokenize(row['correct_sentence']))\n",
    "    row['wrong_word']=' '.join(word_tokenize(row['wrong_word']))\n",
    "    wrong_tokens=word_tokenize(row['wrong_sentence'])\n",
    "    zeros=[0]*len(wrong_tokens)\n",
    "    word_tokens_correct.append(word_tokenize(row['correct_sentence']))\n",
    "    word_tokens_wrong.append(wrong_tokens)\n",
    "    if len(row['wrong_word'].split(\" \"))>1:\n",
    "        for i in range(0,len(wrong_tokens)):\n",
    "            if row['wrong_word'].split(\" \")[0]==wrong_tokens[i]:\n",
    "                if row['wrong_word']==\" \".join(wrong_tokens[i:i+len(row['wrong_word'].split(\" \"))]):\n",
    "                    for index in range (i,i+len(row['wrong_word'].split(\" \"))):\n",
    "                        zeros[index]=errors[row['main_category']]*10+sub_category_1[row['sub_category_1']]      \n",
    "    else:\n",
    "        for i in range(0,len(wrong_tokens)):\n",
    "            if row['wrong_word']==wrong_tokens[i]:\n",
    "                zeros[i]=errors[row['main_category']]*10+sub_category_1[row['sub_category_1']]\n",
    "    if all(x == 0 for x in zeros):    \n",
    "        for i in range(0,len(wrong_tokens)):\n",
    "            under_line_list=wrong_tokens[i].split(\"_\")\n",
    "            if \"_\" in wrong_tokens[i]:\n",
    "                if row['wrong_word']in under_line_list:\n",
    "                    zeros[i]=errors[row['main_category']]*10+sub_category_1[row['sub_category_1']]\n",
    "    if all(x == 0 for x in zeros):    \n",
    "        for i in range(0,len(wrong_tokens)):\n",
    "            if row['wrong_word'] in wrong_tokens[i]:\n",
    "                zeros[i]=errors[row['main_category']]*10+sub_category_1[row['sub_category_1']]\n",
    "    if all(x == 0 for x in zeros):    \n",
    "        if len(row['wrong_word'].split(\" \"))>1:\n",
    "            for i in range(0,len(wrong_tokens)):\n",
    "                if (row['wrong_word'].split(\" \")[0]==wrong_tokens[i] or row['wrong_word'].split(\" \")[0] in wrong_tokens[i]) and i + len(row['wrong_word'].split(\" \"))<=len(wrong_tokens):\n",
    "                    flag=True\n",
    "                    for z in range(1,len(row['wrong_word'].split(\" \"))):\n",
    "                        if row['wrong_word'].split(\" \")[z]!=wrong_tokens[i+z] and row['wrong_word'].split(\" \")[z] not in wrong_tokens[i+1]:\n",
    "                            flag=False\n",
    "                    if flag:\n",
    "                        for index in range (i,i+len(row['wrong_word'].split(\" \"))):\n",
    "                            zeros[index]=errors[row['main_category']]*10+sub_category_1[row['sub_category_1']]\n",
    "    \n",
    "    if all(x==0 for x in zeros):\n",
    "        if row['main_category']==\"Punctuation\" and row['process']==\"Omission\":\n",
    "            zeros[-1]=40\n",
    "    if all(x==0 for x in zeros):\n",
    "        print(row['wrong_sentence'])\n",
    "        print(row)\n",
    "        print()\n",
    "                \n",
    "        \n",
    "    error_index.append(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d5fffcc-0a16-4a6c-bd22-c9eb2b4658c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_tokens_wrong']=word_tokens_wrong\n",
    "df['word_tokens_correct']=word_tokens_correct\n",
    "df['error_index']=error_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "348699f6-a823-4483-b645-a37b7027a02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=0\n",
    "for index , row in df.iterrows():\n",
    "    if all(x == 0 for x in row['error_index']):\n",
    "        s+=1\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45bec816-27c5-446d-bfe4-84bcd4056255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>USER_NUMBER</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>wrong_word</th>\n",
       "      <th>correct_word</th>\n",
       "      <th>wrong_sentence</th>\n",
       "      <th>correct_sentence</th>\n",
       "      <th>process</th>\n",
       "      <th>main_category</th>\n",
       "      <th>sub_category_1</th>\n",
       "      <th>sub_category_2</th>\n",
       "      <th>word_tokens_wrong</th>\n",
       "      <th>word_tokens_correct</th>\n",
       "      <th>error_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n_dialog_11_3.txt</td>\n",
       "      <td>4011744020</td>\n",
       "      <td>140</td>\n",
       "      <td>145</td>\n",
       "      <td>داداش</td>\n",
       "      <td>داداش؟</td>\n",
       "      <td>سلام ، چطوری داداش الحمدلله خوبم انشاء الله</td>\n",
       "      <td>سلام ، چطوری داداش ؟ الحمدلله خوبم انشاء الله</td>\n",
       "      <td>Omission</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[سلام, ،, چطوری, داداش, الحمدلله, خوبم, انشاء,...</td>\n",
       "      <td>[سلام, ،, چطوری, داداش, ؟, الحمدلله, خوبم, انش...</td>\n",
       "      <td>[0, 0, 0, 40, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n_dialog_11_3.txt</td>\n",
       "      <td>4011744020</td>\n",
       "      <td>181</td>\n",
       "      <td>185</td>\n",
       "      <td>خوبم</td>\n",
       "      <td>خوبم.</td>\n",
       "      <td>سلام ، چطوری داداش الحمدلله خوبم انشاء الله</td>\n",
       "      <td>سلام ، چطوری داداش الحمدلله خوبم . انشاء الله</td>\n",
       "      <td>Replacement</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[سلام, ،, چطوری, داداش, الحمدلله, خوبم, انشاء,...</td>\n",
       "      <td>[سلام, ،, چطوری, داداش, الحمدلله, خوبم, ., انش...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 40, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n_dialog_11_3.txt</td>\n",
       "      <td>4011744020</td>\n",
       "      <td>186</td>\n",
       "      <td>191</td>\n",
       "      <td>انشاء</td>\n",
       "      <td>ان شاء</td>\n",
       "      <td>سلام ، چطوری داداش الحمدلله خوبم انشاء الله</td>\n",
       "      <td>سلام ، چطوری داداش الحمدلله خوبم ان شاء الله</td>\n",
       "      <td>Replacement</td>\n",
       "      <td>Form</td>\n",
       "      <td>Boundary</td>\n",
       "      <td>Merging</td>\n",
       "      <td>[سلام, ،, چطوری, داداش, الحمدلله, خوبم, انشاء,...</td>\n",
       "      <td>[سلام, ،, چطوری, داداش, الحمدلله, خوبم, ان, شا...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 31, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n_dialog_11_3.txt</td>\n",
       "      <td>4011744020</td>\n",
       "      <td>198</td>\n",
       "      <td>202</td>\n",
       "      <td>خوبی</td>\n",
       "      <td>خوبی؟</td>\n",
       "      <td>خوبی سلامتی انشاءالله شما هم خوب هستید</td>\n",
       "      <td>خوبی ؟ سلامتی انشاءالله شما هم خوب هستید</td>\n",
       "      <td>Omission</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[خوبی, سلامتی, انشاءالله, شما, هم, خوب, هستید]</td>\n",
       "      <td>[خوبی, ؟, سلامتی, انشاءالله, شما, هم, خوب, هستید]</td>\n",
       "      <td>[40, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n_dialog_11_3.txt</td>\n",
       "      <td>4011744020</td>\n",
       "      <td>203</td>\n",
       "      <td>209</td>\n",
       "      <td>سلامتی</td>\n",
       "      <td>سلامتی؟</td>\n",
       "      <td>خوبی سلامتی انشاءالله شما هم خوب هستید</td>\n",
       "      <td>خوبی سلامتی ؟ انشاءالله شما هم خوب هستید</td>\n",
       "      <td>Omission</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[خوبی, سلامتی, انشاءالله, شما, هم, خوب, هستید]</td>\n",
       "      <td>[خوبی, سلامتی, ؟, انشاءالله, شما, هم, خوب, هستید]</td>\n",
       "      <td>[0, 40, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           file_name USER_NUMBER begin  end wrong_word correct_word  \\\n",
       "0  n_dialog_11_3.txt  4011744020   140  145      داداش       داداش؟   \n",
       "1  n_dialog_11_3.txt  4011744020   181  185       خوبم        خوبم.   \n",
       "2  n_dialog_11_3.txt  4011744020   186  191      انشاء       ان شاء   \n",
       "3  n_dialog_11_3.txt  4011744020   198  202       خوبی        خوبی؟   \n",
       "4  n_dialog_11_3.txt  4011744020   203  209     سلامتی      سلامتی؟   \n",
       "\n",
       "                                wrong_sentence  \\\n",
       "0  سلام ، چطوری داداش الحمدلله خوبم انشاء الله   \n",
       "1  سلام ، چطوری داداش الحمدلله خوبم انشاء الله   \n",
       "2  سلام ، چطوری داداش الحمدلله خوبم انشاء الله   \n",
       "3       خوبی سلامتی انشاءالله شما هم خوب هستید   \n",
       "4       خوبی سلامتی انشاءالله شما هم خوب هستید   \n",
       "\n",
       "                                correct_sentence      process main_category  \\\n",
       "0  سلام ، چطوری داداش ؟ الحمدلله خوبم انشاء الله     Omission   Punctuation   \n",
       "1  سلام ، چطوری داداش الحمدلله خوبم . انشاء الله  Replacement   Punctuation   \n",
       "2   سلام ، چطوری داداش الحمدلله خوبم ان شاء الله  Replacement          Form   \n",
       "3       خوبی ؟ سلامتی انشاءالله شما هم خوب هستید     Omission   Punctuation   \n",
       "4       خوبی سلامتی ؟ انشاءالله شما هم خوب هستید     Omission   Punctuation   \n",
       "\n",
       "  sub_category_1 sub_category_2  \\\n",
       "0           None           None   \n",
       "1           None           None   \n",
       "2       Boundary        Merging   \n",
       "3           None           None   \n",
       "4           None           None   \n",
       "\n",
       "                                   word_tokens_wrong  \\\n",
       "0  [سلام, ،, چطوری, داداش, الحمدلله, خوبم, انشاء,...   \n",
       "1  [سلام, ،, چطوری, داداش, الحمدلله, خوبم, انشاء,...   \n",
       "2  [سلام, ،, چطوری, داداش, الحمدلله, خوبم, انشاء,...   \n",
       "3     [خوبی, سلامتی, انشاءالله, شما, هم, خوب, هستید]   \n",
       "4     [خوبی, سلامتی, انشاءالله, شما, هم, خوب, هستید]   \n",
       "\n",
       "                                 word_tokens_correct  \\\n",
       "0  [سلام, ،, چطوری, داداش, ؟, الحمدلله, خوبم, انش...   \n",
       "1  [سلام, ،, چطوری, داداش, الحمدلله, خوبم, ., انش...   \n",
       "2  [سلام, ،, چطوری, داداش, الحمدلله, خوبم, ان, شا...   \n",
       "3  [خوبی, ؟, سلامتی, انشاءالله, شما, هم, خوب, هستید]   \n",
       "4  [خوبی, سلامتی, ؟, انشاءالله, شما, هم, خوب, هستید]   \n",
       "\n",
       "                 error_index  \n",
       "0  [0, 0, 0, 40, 0, 0, 0, 0]  \n",
       "1  [0, 0, 0, 0, 0, 40, 0, 0]  \n",
       "2  [0, 0, 0, 0, 0, 0, 31, 0]  \n",
       "3     [40, 0, 0, 0, 0, 0, 0]  \n",
       "4     [0, 40, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b67da00e-fa15-4a32-8529-abe4d173bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data_temp.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e779b282-09c9-478d-a7b4-f2d9648f4d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('data_temp.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4cfe19-79f0-4d06-9f75-7ec25300e3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
